Token：1个Token约等于1.5-2个汉字

文本模型的Input和Output的总和长度不能超过模型的最大上下文长度

SoTA：state of the art的缩写，在AI领域指的是在特定任务领域中最先进的模型

xxxK上下文中的K：1024

Benchmark：大模型的基础测试，用于评估大模型性能的一系列标准化任务、数据集、评价指标

模型压测：性能测试、功能测试、精度测试

部署后模型的一次推理响应分为下面两个阶段，

预填充阶段：处理用户输入的整个提示词，生成第一个输出词元，这是一个计算密集型过程

解码阶段：基于已生成的词元，逐个预测并生成后续词元，这是一个内存带宽密集型过程

模型部署后用来衡量模型推理速度、吞吐量的关键性能指标：

TPM：Tokens Per Minute，每分钟处理的总词元数（包括输入输出）

RPM：Requests Per Minute，每分钟处理的请求数量

TTFT：Time To First Token，从用户发送完整请求到收到模型生成的第一个词元所花费的时间，对应预填充阶段，TTFT越短用户感觉响应越快

TPOT：Time Per Output Token，从收到第一个词元开始，到后续每个词元输出的平均生成时间，对应解码阶段，TPOT越低输出流式感越强

以下两个指标剥离了系统排队与并发干扰，用于评估模型的理论极限性能

No-load TTFT：无负载首词元时间，在系统完全空闲情况下测得的TTFT

No-load TPOT：无负载单请求词元时间，在系统完全空闲情况下测得的TPOT



# 模型参数

B：十亿，8B、10B这些是用来描述模型参数量

每个参数的精度系数为该参数在计算机内存中所占用的字节数，有如下几种

FP32全精度，每个参数占用4字节4B

FP16/BF16半精度，每个参数占用2字节2B

INT8八位量化，每个参数占1字节1B，

INT4四位量化，每个参数占0.5字节0.5B

一个模型的显存占用为：参数量*精度系数



**温度**

浮点类型，每次可调整0.01，最大1.99。

核采样阈值。用于决定结果随机性，取值越高随机性越强即相同的问题得到的不同答案的可能性越高。

**Top P**

浮点类型，每次可调整0.01，最大1，最小0.01。

生成过程中核采样方法概率阈值。取值越大，生成的随机性越高；取值越小，生成的确定性越高。

**存在惩罚**

浮点类型，每次可调整0.01，最大2，最小-2。

用于控制模型生成时的重复度。提高此项可以降低模型生成的重复度。



# 推理引擎

类比web应用，写好的代码不能直接运行，需要借助nginx部署到web服务器上。而训练得到的模型文件类比写好的代码，需要借助推理引擎运行。



## VLLM





## SGLang







# 强化学习RL（Reinforcement learing）

DQN（Deep Q-Networks）

TRPO（Trust Region Policy Optmization）

近端策略优化PPO（Proximal Policy Optimization）



# ReAct: 在语言模型中协同推理与行动

虽然大语言模型（LLMs）在语言理解和交互决策任务中展现出了令人印象深刻的性能，但它们的推理能力（例如思维链提示）和行动能力（例如行动计划生成）在很大程度上一直被作为两个独立的课题进行研究。在本文中，我们探索了使用大语言模型以**交替（interleaved）\**的方式生成\**推理轨迹（reasoning traces）\**和\**特定任务行动**的方法，从而实现两者之间更强大的协同作用：推理轨迹可以帮助模型归纳、跟踪和更新行动计划，并处理异常情况；而行动则允许模型与外部源（如知识库或环境）对接，并从中获取额外信息。

我们将这种命名为 **ReAct** 的方法应用于多种语言和决策任务，证明了其效果优于目前最先进的基准模型，并提高了人类的可解释性和可信度。具体而言，在问答（HotpotQA）和事实核查（Fever）任务中，ReAct 通过与简单的维基百科 API 交互，克服了思维链推理中普遍存在的**幻觉（hallucination）\**和\**错误传播**问题，并生成了类人化的任务解决轨迹，比没有推理轨迹的基准模型更具可解释性。此外，在两个交互式决策基准测试（ALFWorld 和 WebShop）中，ReAct 在仅需一两个上下文示例（in-context examples）进行提示的情况下，表现优于模仿学习和强化学习方法，绝对成功率分别提升了 34% 和 10%。











